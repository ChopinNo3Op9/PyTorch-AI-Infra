{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --extra-index-url https://developer.download.nvidia.com/compute/redist nvidia-dali-cuda110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "from nvidia.dali.pipeline import Pipeline\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# DALI pipeline for CIFAR-10\n",
    "class CIFAR10Pipeline(Pipeline):\n",
    "    def __init__(self, batch_size, num_threads, device_id, data_dir, train=True):\n",
    "        super(CIFAR10Pipeline, self).__init__(batch_size, num_threads, device_id)\n",
    "        self.input = fn.readers.caffe2(path=data_dir, random_shuffle=train, name=\"Reader\")\n",
    "        \n",
    "        # Define pipeline operations\n",
    "        self.decode = fn.decoders.image(self.input, device=\"mixed\", output_type=types.RGB)\n",
    "        self.resize = fn.resize(self.decode, resize_x=224, resize_y=224)\n",
    "        self.cmnp = fn.crop_mirror_normalize(self.resize, dtype=types.FLOAT,\n",
    "                                             mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "                                             std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "        self.uniform = fn.random.uniform(range=(0.0, 1.0))\n",
    "        self.mirror = fn.cast(self.uniform > 0.5, dtype=types.DALIDataType.BOOL)\n",
    "\n",
    "    def define_graph(self):\n",
    "        inputs, labels = self.input(name=\"Reader\")\n",
    "        images = self.cmnp(self.resize(self.decode(inputs)), mirror=self.mirror)\n",
    "        return images, labels\n",
    "\n",
    "# Initialize the DALI pipeline\n",
    "batch_size = 32\n",
    "cifar10_pipeline = CIFAR10Pipeline(batch_size=batch_size, num_threads=2, device_id=0,\n",
    "                                   data_dir=\"/path/to/cifar10\", train=True)\n",
    "cifar10_pipeline.build()\n",
    "\n",
    "# Create a DALI iterator for PyTorch\n",
    "dali_iterator = DALIClassificationIterator(cifar10_pipeline, last_batch_policy=LastBatchPolicy.PARTIAL)\n",
    "\n",
    "# Initialize ResNet18\n",
    "net = resnet18(pretrained=False)\n",
    "net.fc = nn.Linear(net.fc.in_features, 10)  # Adjust for CIFAR-10\n",
    "\n",
    "# Training essentials\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dali_iterator):\n",
    "        # get the inputs; data is a dict with 'data' and 'label'\n",
    "        inputs, labels = data[0][\"data\"], data[0][\"label\"].squeeze().long()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    dali_iterator.reset()\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Cleanup\n",
    "dali_iterator.shutdown()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
