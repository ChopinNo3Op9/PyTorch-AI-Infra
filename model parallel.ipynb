{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ResNet18ModelParallel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18ModelParallel, self).__init__()\n",
    "        original_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Split the model at the convolutional part and the fully connected part\n",
    "        self.part1 = nn.Sequential(\n",
    "            *list(original_model.children())[:-2]\n",
    "        ).to('cuda:0')  # Assuming you have at least two GPUs\n",
    "        \n",
    "        self.part2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            *list(original_model.children())[-2:]\n",
    "        ).to('cuda:1')  # Second part on the second GPU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.part1(x.to('cuda:0'))  # Move input to GPU 0 and forward through part1\n",
    "        x = x.to('cuda:1')  # Move intermediate output to GPU 1\n",
    "        x = self.part2(x)  # Forward through part2 on GPU 1\n",
    "        return x\n",
    "\n",
    "def main():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model = ResNet18ModelParallel()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to('cuda:1')  # Assuming the loss calculation happens on the second GPU\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.to('cuda:1'))  # Move labels to GPU 1 where the output resides\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f'Loss: {loss.item()}')\n",
    "                \n",
    "    print('Finished Training')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuda:0: The first part of the ResNet18 model, consisting of the convolutional layers, is placed on the first GPU. This device is used for processing the initial stages of the model's forward pass.  \n",
    "\n",
    "cuda:1: The second part of the model, including the adaptive average pooling, flattening, and the fully connected layers, is placed on the second GPU. This device is used for completing the forward pass and for calculating the loss and backpropagation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
