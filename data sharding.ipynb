{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Load CIFAR10 dataset\n",
    "    train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "    \n",
    "    # Create a DistributedSampler to shard the dataset\n",
    "    train_sampler = DistributedSampler(train_set, num_replicas=world_size, rank=rank)   # world_size = number of GPUs\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=64, sampler=train_sampler)  # fetches the respective subset for each GPU.\n",
    "    \n",
    "    # Define your model and move it to the GPU\n",
    "    model = resnet18().to(rank)\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(rank), labels.to(rank)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Rank {rank}, Epoch {epoch}, Batch {i}, Loss {loss.item()}')\n",
    "    \n",
    "    cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 2  # Number of processes to spawn. Adjust according to your GPU count.\n",
    "    mp.spawn(main, args=(world_size,), nprocs=world_size, join=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
